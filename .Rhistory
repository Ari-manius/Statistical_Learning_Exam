x = X,
y = y,
method = custom_nn,  # Custom neural network
trControl = train_control,
metric = "ROC"
)
# Train and evaluate Random Forest
rf_model <- caret::train(
x = X,
y = y,
method = "rf",  # Random Forest
trControl = train_control,
metric = "ROC"
)
# Train and evaluate KNN
knn_model <- caret::train(
x = X,
y = y,
method = "knn",  # K-Nearest Neighbors
trControl = train_control,
metric = "ROC"
)
# Collect results
results <- resamples(list(
NeuralNetwork = nn_model,
RandomForest = rf_model,
KNN = knn_model
))
# Summarize results
summary(results)
# Visualize results
bwplot(results)
dotplot(results)
library(caret)
library(keras)
library(tensorflow)
# Set seed for reproducibility
set.seed(42)
# Prepare the data
n <- 1000
X <- as.data.frame(matrix(runif(n * 10), nrow = n, ncol = 10))  # Convert to data frame
y <- as.factor(sample(c("Class0", "Class1"), n, replace = TRUE))  # Binary classification with valid factor levels
# Define a custom neural network model for caret
custom_nn <- list(
library = "keras",
type = "Classification",
parameters = data.frame(parameter = c("layer1", "layer2", "layer3", "hidden_dropout", "visible_dropout"),
class = rep("numeric", 5),
label = c("Layer 1 Units", "Layer 2 Units", "Layer 3 Units", "Hidden Dropout", "Visible Dropout")),
grid = function(x, y, len = NULL, search = "grid") {
data.frame(layer1 = 10, layer2 = 0, layer3 = 0, hidden_dropout = 0, visible_dropout = 0)
},
fit = function(x, y, wts, param, lev, last, weights, classProbs, ...) {
model <- keras_model_sequential() %>%
layer_dense(units = param$layer1, input_shape = ncol(x), activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
model %>% fit(
x = as.matrix(x),
y = as.numeric(y) - 1,
epochs = 10,
batch_size = 32,
verbose = 0,
...
)
return(model)
},
predict = function(modelFit, newdata, preProc = NULL, submodels = NULL) {
pred <- predict(modelFit, as.matrix(newdata))
pred <- ifelse(pred > 0.5, "Class1", "Class0")
return(pred)
},
prob = function(modelFit, newdata, preProc = NULL, submodels = NULL) {
pred <- predict(modelFit, as.matrix(newdata))
prob <- data.frame(Class0 = 1 - pred, Class1 = pred)
return(prob)
}
)
# Set up k-fold cross-validation
k <- 5
train_control <- caret::trainControl(
method = "cv",  # k-fold cross-validation
number = k,     # Number of folds
savePredictions = "final",
classProbs = TRUE,
summaryFunction = twoClassSummary
)
# Train and evaluate Neural Network
nn_model <- caret::train(
x = X,
y = y,
method = custom_nn,  # Custom neural network
trControl = train_control,
metric = "ROC"
)
# Train and evaluate Random Forest
rf_model <- caret::train(
x = X,
y = y,
method = "rf",  # Random Forest
trControl = train_control,
metric = "ROC"
)
# Train and evaluate KNN
knn_model <- caret::train(
x = X,
y = y,
method = "knn",  # K-Nearest Neighbors
trControl = train_control,
metric = "ROC"
)
# Collect results
results <- resamples(list(
NeuralNetwork = nn_model,
RandomForest = rf_model,
KNN = knn_model
))
# Summarize results
summary(results)
# Visualize results
bwplot(results)
dotplot(results)
knitr::opts_chunk$set(error = TRUE)
reticulate::use_python("/Users/ramius/.pyenv/versions/3.10.0/bin")
#| message: false
#| warning: false
#packages used:
library(readr)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(caret)
library(randomForest)
library(torch)
library(tensorflow)
library(luz)
library(reshape2)
library(pROC)
library(ggplot2)
library(class)
torch_manual_seed(33)
set.seed(33)
#| message: false
#| warning: false
# Load and preprocess data
df <- read_csv("/Users/ramius/Desktop/CodeVault/01_Project/Uni/SL_StatisticalLearning/SL Exam/apple_quality.csv")
df <- as.data.frame(df)
df <- df[1:4000, ] # exclude last NA value
df$Quality <- as.integer(df$Quality == "good")  # 1 is good, 0 is bad
df <- df[, 2:9]  # Drop unique ID's
# Split into features (X) and target (y)
X <- as.matrix(df[, -ncol(df)])  # All columns except the last because target
X <- apply(X, 2, as.numeric)     # Ensure all columns are numeric
X <- scale(X,center = TRUE, scale = TRUE)                    # Standardize features
y <- df[, ncol(df)]              # Last column is the target
#| message: false
#| warning: false
# test / train split
train_indices <- sample(1:nrow(X), size = 0.75 * nrow(X))
test_indices <- setdiff(1:nrow(X), train_indices)
# For torch
X_tensor <- torch_tensor(X, dtype = torch_float32())
y_tensor <- torch_tensor(y, dtype = torch_float32())$unsqueeze(2)
X_train <- X_tensor[train_indices, ]
y_train <- y_tensor[train_indices, ]
X_test <- X_tensor[test_indices, ]
y_test <- y_tensor[test_indices, ]
# For random forest
y_factor <- as.factor(y)
train_df <- data.frame(Target = y_factor[train_indices], X[train_indices, ])
test_df <- data.frame(Target = y_factor[test_indices], X[test_indices, ])
model <- nn_module(
initialize = function(input_size) {
self$input <- nn_linear(input_size, 256)
self$fc1 <- nn_linear(256, 128)
self$fc2 <- nn_linear(128, 64)
self$fc3 <- nn_linear(64, 16)
self$output <- nn_linear(16, 1)
self$dropout <- nn_dropout(0.2) # neurons in that layer will be randomly turned off in each training step.(prevent overfitting)
},
forward = function(x) {
x %>%
self$input() %>% nnf_relu() %>% self$dropout() %>%
self$fc1() %>% nnf_relu() %>% self$dropout() %>%
self$fc2() %>% nnf_relu() %>% self$dropout() %>%
self$fc3() %>% nnf_relu() %>%
self$output() %>% nnf_sigmoid()
}
)
num_epochs <- 250 #maximum number of epochs
train_loss_history <- numeric()
val_loss_history <- numeric()
train_accuracy_history <- numeric()
test_accuracy_history <- numeric()
model <- model(input_size = ncol(X))
criterion <- nn_bce_loss()  # Binary classification loss function
optimizer <- optim_adam(model$parameters, lr = 0.001, weight_decay = 0.01) # learning rate that is not too high and not too low
batch_size <- 32 # reasonable batch size
num_batches <- ceiling(nrow(X_train) / batch_size)
# Early stopping parameters
patience <- 10  # Number of epochs to wait before stopping
best_val_loss <- Inf
epochs_without_improvement <- 0
best_model_state <- NULL
for (epoch in 1:num_epochs) {
model$train()
total_loss <- 0
# Shuffle dataset
perm <- sample(1:nrow(X_train))
X_train_shuffled <- X_train[perm, ]
y_train_shuffled <- y_train[perm]
# batch training and learning
for (i in 1:num_batches) {
start_idx <- (i - 1) * batch_size + 1
end_idx <- min(i * batch_size, nrow(X_train))
X_batch <- torch_tensor(as.matrix(X_train_shuffled[start_idx:end_idx, ]), dtype = torch_float())
y_batch <- torch_tensor(as.numeric(y_train_shuffled[start_idx:end_idx]), dtype = torch_float())$view(c(-1, 1))
optimizer$zero_grad()
outputs <- model(X_batch)
loss <- criterion(outputs, y_batch)
loss$backward()
optimizer$step()
total_loss <- total_loss + loss$item()
}
# model evaluation and validation
avg_train_loss <- total_loss / num_batches
train_loss_history[epoch] <- avg_train_loss
model$eval()
val_outputs <- model(torch_tensor(as.matrix(X_test), dtype = torch_float()))
val_loss <- criterion(val_outputs, torch_tensor(as.numeric(y_test), dtype = torch_float())$view(c(-1, 1)))
val_loss_history[epoch] <- val_loss$item()
train_accuracy_history[epoch] <- mean(as.factor(round(as.numeric(model(X_train)))) == as.factor(as.numeric(y_train)))
test_accuracy_history[epoch] <- mean(as.factor(round(as.numeric(model(X_test)))) == as.factor(as.numeric(y_test)))
# early stopping when learning stops
if (val_loss$item() < best_val_loss) {
best_val_loss <- val_loss$item()
epochs_without_improvement <- 0
best_model_state <- model$state_dict()
} else {
epochs_without_improvement <- epochs_without_improvement + 1
}
cat(sprintf("Epoch %d: Train Loss = %.4f | Val Loss = %.4f\n", epoch, avg_train_loss, val_loss$item()))
if (epochs_without_improvement >= patience) {
cat("Stopping early! No improvement for", patience, "epochs.\n")
break
}
}
# Restore the best model
if (!is.null(best_model_state)) {
model$load_state_dict(best_model_state)
cat("Restored the best model from epoch with lowest validation loss.\n")
}
#| message: false
#| warning: false
history <- data.frame(
epoch = 1:length(train_loss_history),
train_loss = train_loss_history,
val_loss = val_loss_history,
train_accuracy = train_accuracy_history,
test_accuracy = test_accuracy_history
)
df_melt <- melt(history, id.vars = "epoch",
variable.name = "metric", value.name = "value")
ggplot(df_melt, aes(x = epoch, y = value, color = metric)) +
geom_line(size = 1.2) +
labs(title = "Training & Validation Loss & Accuracy of Neural Network", y = "Value", x = "Epoch") +
scale_color_manual(values = c("train_loss" = "blue", "val_loss" = "red",
"train_accuracy" = "green", "test_accuracy" = "purple"),
labels = c("Train Loss", "Validation Loss",
"Train Accuracy", "Validation Accuracy")) +
theme_dark()
confusion_train <- confusionMatrix(as.factor(round(as.numeric(model(X_train)))), as.factor(as.numeric(y_train)))
confusion_test <- confusionMatrix(as.factor(round(as.numeric(model(X_test)))), as.factor(as.numeric(y_test)))
confusion_train_df <- as.data.frame(confusion_train$table)
confusion_test_df <- as.data.frame(confusion_test$table)
confusion_train_df$Data <- "Training"
confusion_test_df$Data <- "Test"
confusion_df <- rbind(confusion_train_df, confusion_test_df)
# Create a heatmap for the confusion matrices
ggplot(confusion_df, aes(x = Reference, y = Prediction, fill = Freq)) +
geom_tile() +
geom_text(aes(label = Freq), color = "white", size = 5) +
scale_fill_gradient(low = "blue", high = "red") +
facet_wrap(~ Data) +
labs(title = "Confusion Matrices for Neural Network",
x = "Reference",
y = "Prediction",
fill = "Frequency") +
theme_dark() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Neural Network Evaluation
model$eval()
with_no_grad({
test_preds <- model(X_tensor[test_indices, ])
test_probs <- as.numeric(test_preds)
test_labels <- y[test_indices]
})
# Calculate ROC object
roc_obj <- roc(test_labels, test_probs)
# Extract ROC curve data for ggplot
roc_data <- data.frame(
Sensitivity = roc_obj$sensitivities,
Specificity = roc_obj$specificities,
Thresholds = roc_obj$thresholds
)
# Plot ROC curve using ggplot2
ggplot(roc_data, aes(x = 1 - Specificity, y = Sensitivity)) +
geom_line(color = "blue", size = 1.2) +  # ROC curve line
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red", size = 1) +  # Diagonal reference line
labs(
title = "ROC Curve",
x = "False Positive Rate (1 - Specificity)",
y = "True Positive Rate (Sensitivity)",
caption = paste("AUC =", round(auc(roc_obj), 3))  # Add AUC value to the caption
) +
theme_dark() +
theme(
plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
axis.title = element_text(size = 14),
axis.text = element_text(size = 12),
panel.grid.major = element_line(color = "gray90"),
panel.grid.minor = element_blank(),
plot.caption = element_text(size = 12, face = "italic", hjust = 1)
) +
scale_x_continuous(expand = c(0, 0)) +  # Remove extra space on x-axis
scale_y_continuous(expand = c(0, 0))    # Remove extra space on y-axis
rf_model <- randomForest(
Target ~ .,               # Formula (Target as a function of all features)
data = train_df,          # Training data
do.trace = FALSE,         # No detailed logging
ntree = 500,              # Number of trees (adjustable)
mtry = sqrt(ncol(train_df) - 1),  # Features considered at each split
importance = TRUE,        # Calculate feature importance
proximity = TRUE          # Compute proximity matrix (optional)
)
# KNN Classifier
knn_model <- knn(
train = train_df[, -which(names(train_df) == "Target")],  # Training features (exclude Target)
test = test_df[, -which(names(test_df) == "Target")],     # Test features (exclude Target)
cl = train_df$Target,                                     # Training labels
k = 5,                                                    # Number of neighbors (adjustable)
prob = FALSE                                               # Return probabilities
)
confusion_rf <- confusionMatrix(predict(rf_model, test_df), test_df$Target)
confusion_knn <- confusionMatrix(knn_model, test_df$Target)
NeuralNetworkTrained <- confusion_train$byClass
NeuralNetworkValidated <- confusion_test$byClass
RandomForest <- confusion_rf$byClass
KNN <- confusion_knn$byClass
# Combine the data frames using cbind
combined_metrics_df <- cbind(
#Train = as.data.frame(NeuralNetworkTrained),
Test = as.data.frame(NeuralNetworkValidated),
RF = as.data.frame(RandomForest),
KNN = as.data.frame(KNN)
)
# Add a "Metric" column from row names
combined_metrics_df <- combined_metrics_df %>%
tibble::rownames_to_column(var = "Metric")
# Convert to long format
long_metrics_df <- combined_metrics_df %>%
pivot_longer(
cols = -Metric,  # Keep the "Metric" column
names_to = "Data",  # Name of the new column for Train, Test, RF
values_to = "Value"  # Name of the new column for metric values
)
# Plot the data
ggplot(long_metrics_df, aes(x = Metric, y = Value, fill = Data)) +
geom_bar(stat = "identity", position = "dodge") +
labs(
title = "Model Evaluation Metrics",
x = "Metric",
y = "Value",
fill = "Data"
) +
theme_dark() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
#| echo: false
#| message: false
#| warning: false
df_numeric <- df %>% select(-Quality)
df_numeric <- df_numeric %>%
mutate_all(~ifelse(is.na(.), mean(., na.rm = TRUE), .)) %>%
mutate_all(~ifelse(is.infinite(.), max(., na.rm = TRUE), .))
df_pca <- prcomp(df_numeric, center = TRUE, scale. = TRUE)
pca_df <- data.frame(df_pca$x[, 1:2], Target = as.factor(df$Quality))
colnames(pca_df) <- c("PC1", "PC2", "Target")
# Scree Plot: Variance explained by each principal component
explained_var <- df_pca$sdev^2 / sum(df_pca$sdev^2) * 100
scree_df <- data.frame(PC = seq_along(explained_var), Variance = explained_var)
# PCA Loadings: Most important variables for PC1 and PC2
loadings <- as.data.frame(df_pca$rotation[, 1:2])
loadings$Feature <- rownames(loadings)
top_features_pc1 <- loadings %>% arrange(desc(abs(PC1))) %>% head(5)
top_features_pc2 <- loadings %>% arrange(desc(abs(PC2))) %>% head(5)
ggplot(scree_df, aes(x = PC, y = Variance)) +
geom_bar(stat = "identity", fill = "steelblue", alpha = 0.7) +
geom_line(aes(group = 1), color = "black") +
geom_point(color = "red", size = 2) +
labs(title = "Scree Plot", x = "Principal Component", y = "Explained Variance (%)") +
theme_dark()
ggplot(pca_df, aes(x = PC1, y = PC2, color = Target)) +
geom_point(alpha = 0.5, size = 1) +
labs(title = "PCA Result for Apple Dataset", x = "Principal Component 1", y = "Principal Component 2") +
theme_dark() +
scale_color_discrete(name = "Quality")
# Visualizing the top contributing features
ggplot(loadings, aes(x = PC1, y = PC2, label = Feature)) +
geom_vline(xintercept = 0, linetype = "dashed", color = "gray") +
geom_hline(yintercept = 0, linetype = "dashed", color = "gray") +
geom_point(color = "blue", alpha = 0.6) +
geom_text_repel(size = 4, color = "black") +
labs(title = "PCA Loadings: Top Variables for PC1 & PC2",
x = "PC1 Contribution", y = "PC2 Contribution") +
theme_dark()
library(caret)
library(keras)
library(tensorflow)
# Set seed for reproducibility
set.seed(42)
# Prepare the data
n <- 1000
X <- as.data.frame(matrix(runif(n * 10), nrow = n, ncol = 10))  # Convert to data frame
y <- as.factor(sample(c("Class0", "Class1"), n, replace = TRUE))  # Binary classification with valid factor levels
# Define a custom neural network model for caret
custom_nn <- list(
library = "keras",
type = "Classification",
parameters = data.frame(parameter = c("layer1", "layer2", "layer3", "hidden_dropout", "visible_dropout"),
class = rep("numeric", 5),
label = c("Layer 1 Units", "Layer 2 Units", "Layer 3 Units", "Hidden Dropout", "Visible Dropout")),
grid = function(x, y, len = NULL, search = "grid") {
data.frame(layer1 = 10, layer2 = 0, layer3 = 0, hidden_dropout = 0, visible_dropout = 0)
},
fit = function(x, y, wts, param, lev, last, weights, classProbs, ...) {
model <- keras_model_sequential() %>%
layer_dense(units = param$layer1, input_shape = ncol(x), activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
model %>% fit(
x = as.matrix(x),
y = as.numeric(y) - 1,
epochs = 10,
batch_size = 32,
verbose = 0,
...
)
return(model)
},
predict = function(modelFit, newdata, preProc = NULL, submodels = NULL) {
pred <- predict(modelFit, as.matrix(newdata))
pred <- ifelse(pred > 0.5, "Class1", "Class0")
return(pred)
},
prob = function(modelFit, newdata, preProc = NULL, submodels = NULL) {
pred <- predict(modelFit, as.matrix(newdata))
prob <- data.frame(Class0 = 1 - pred, Class1 = pred)
return(prob)
}
)
# Set up k-fold cross-validation
k <- 5
train_control <- caret::trainControl(
method = "cv",  # k-fold cross-validation
number = k,     # Number of folds
savePredictions = "final",
classProbs = TRUE,
summaryFunction = twoClassSummary
)
# Train and evaluate Neural Network
nn_model <- caret::train(
x = X,
y = y,
method = custom_nn,  # Custom neural network
trControl = train_control,
metric = "ROC"
)
# Train and evaluate Random Forest
rf_model <- caret::train(
x = X,
y = y,
method = "rf",  # Random Forest
trControl = train_control,
metric = "ROC"
)
# Train and evaluate KNN
knn_model <- caret::train(
x = X,
y = y,
method = "knn",  # K-Nearest Neighbors
trControl = train_control,
metric = "ROC"
)
# Collect results
results <- resamples(list(
NeuralNetwork = nn_model,
RandomForest = rf_model,
KNN = knn_model
))
# Summarize results
summary(results)
# Visualize results
bwplot(results)
dotplot(results)
